{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\00006537328c33e284c973d7b39d340809f7271b.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\0000ec92553fda4ce39889f9226ace43cae3364e.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\00024a6dee61f12f7856b0fc6be20bc7a48ba3d2.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\000253dfaa0be9d0d100283b22284ab2f6b643f6.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\000270442cc15af719583a8172c87cd2bd9c7746.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\000309e669fa3b18fb0ed6a253a2850cce751a95.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\000360e0d8358db520b5c7564ac70c5706a0beb0.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\00040095a4a671280aeb66cb0c9231e6216633b5.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\000698b7df308d75ec9559ef473a588c513a68aa.tif', 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\0006e1af5670323331d09880924381d67d79eda0.tif']\n",
      "Indexes: 0 - 5000\n",
      "Indexes: 5000 - 10000\n",
      "Indexes: 10000 - 15000\n",
      "Indexes: 15000 - 20000\n",
      "Indexes: 20000 - 25000\n",
      "Indexes: 25000 - 30000\n",
      "Indexes: 30000 - 35000\n",
      "Indexes: 35000 - 40000\n",
      "Indexes: 40000 - 45000\n",
      "Indexes: 45000 - 50000\n",
      "Indexes: 50000 - 55000\n",
      "Indexes: 55000 - 57458\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TU/e BME Project Imaging 2019\n",
    "Submission code for Kaggle PCAM\n",
    "Author: Suzanne Wetstein\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "TEST_PATH = 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\data\\\\test\\\\'\n",
    "MODEL_FILEPATH = 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\GitHub\\\\code\\\\my_first_cnn_model.json'\n",
    "MODEL_WEIGHTS_FILEPATH = 'C:\\\\Users\\\\s167917\\\\Documents\\\\#School\\\\Jaar 3\\\\3 Project Imaging\\\\GitHub\\\\code\\\\my_first_cnn_model_weights.hdf5'\n",
    "\n",
    "# load model and model weights\n",
    "json_file = open(MODEL_FILEPATH, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(MODEL_WEIGHTS_FILEPATH)\n",
    "\n",
    "\n",
    "# open the test set in batches (as it is a very big dataset) and make predictions\n",
    "test_files = glob.glob(TEST_PATH + '*.tif')\n",
    "print(test_files[0:10])\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "file_batch = 5000\n",
    "max_idx = len(test_files)\n",
    "\n",
    "for idx in range(0, max_idx, file_batch):\n",
    "\n",
    "    print('Indexes: %i - %i'%(idx, min(idx+file_batch,max_idx)))\n",
    "\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
    "\n",
    "\n",
    "    # get the image id \n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split(os.sep)[-1].split('.')[0])\n",
    "    test_df['image'] = test_df['path'].map(imread)\n",
    "    \n",
    "    \n",
    "    K_test = np.stack(test_df['image'].values)\n",
    "    \n",
    "    # apply the same preprocessing as during draining\n",
    "    K_test = K_test.astype('float')/255.0\n",
    "    \n",
    "    predictions = model.predict(K_test)\n",
    "    \n",
    "    test_df['label'] = predictions\n",
    "    submission = pd.concat([submission, test_df[['id', 'label']]])\n",
    "\n",
    "\n",
    "# save your submission\n",
    "submission.head()\n",
    "submission.to_csv('submission.csv', index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
